{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GE5tnx0t21ZO"
   },
   "source": [
    "# Feature engineering for transactions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PJHWHbWB0AEG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ryUGXyu0JT3",
    "outputId": "f9ce73bf-2422-47cc-9d24-b3e50eb58ef8"
   },
   "outputs": [],
   "source": [
    "path = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5r-dngWa0Jxa"
   },
   "outputs": [],
   "source": [
    "transactions_train = pd.read_csv(path + \"transactions_train.csv\")\n",
    "transactions_test = pd.read_csv(path + \"transactions_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pBJ2NAa60en-"
   },
   "outputs": [],
   "source": [
    "def downsample(transactions, verbose=False):\n",
    "\n",
    "    # Downsamples our data so it's not as memory intensive as it could be.\n",
    "    # Verbose=True prints the results of downsampling\n",
    "\n",
    "    if verbose:\n",
    "        print('before downsampling:\\n', transactions.dtypes)\n",
    "    transactions['client_id'] = pd.to_numeric(transactions['client_id'], downcast='unsigned')\n",
    "    transactions['trans_date'] = pd.to_numeric(transactions['trans_date'], downcast='unsigned')\n",
    "    transactions['small_group'] = pd.to_numeric(transactions['small_group'], downcast='signed')\n",
    "    transactions['amount_rur'] = pd.to_numeric(transactions['amount_rur'], downcast='float')\n",
    "    if verbose:\n",
    "        print('after downsampling:\\n',transactions.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9cDVfmJr0zah"
   },
   "outputs": [],
   "source": [
    "def filter_groups(transactions, threshold_value=0.01, based_on_volume=True):\n",
    "    \n",
    "    # Filters some of the rare vendor groups\n",
    "    # If based_on_volume is true, filters based on transaction volume of the vendor,\n",
    "    # otherwise filters based on the number of transactions of the vendor.\n",
    "\n",
    "    if based_on_volume:\n",
    "        normalized_sums = transactions.groupby('small_group').sum()['amount_rur']\n",
    "        normalized_sums /= normalized_sums.sum()\n",
    "        normalized_sorted_sums = normalized_sums.sort_values()\n",
    "\n",
    "        threshold = normalized_sorted_sums[(normalized_sorted_sums.cumsum() > threshold_value).idxmax()]\n",
    "\n",
    "        transactions['small_group_filtered'] = transactions['small_group'].mask(transactions['small_group'].map(normalized_sums) < threshold, -1)\n",
    "    else:\n",
    "        frequencies = transactions['small_group'].value_counts(normalize=True, ascending=True)\n",
    "\n",
    "        threshold = frequencies[(frequencies.cumsum() > threshold_value).idxmax()]\n",
    "        transactions['small_group_filtered'] = transactions['small_group'].mask(transactions['small_group'].map(transactions['small_group'].value_counts(normalize=True)) < threshold, -1)\n",
    "    \n",
    "    transactions['small_group'] = transactions['small_group_filtered']\n",
    "    transactions = transactions.drop(columns=['small_group_filtered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "H7ygv9aU2P8j"
   },
   "outputs": [],
   "source": [
    "def flag_large_transactions(transactions, quantile):\n",
    "\n",
    "    # Adds a \"large_transaction\" feature with flags on transactions that have a larger volume than the specified quantile.\n",
    "    # For reference, quantile of 0.996 sets flags on transactions larger than 1000 RUB\n",
    "\n",
    "    transactions['large_amount'] = False\n",
    "    transactions['large_amount'] = transactions['large_amount'].mask(transactions['amount_rur'] > transactions['amount_rur'].quantile(quantile), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "syQ3QMYP3G-q"
   },
   "outputs": [],
   "source": [
    "def group_mean(transactions):\n",
    "\n",
    "    # Simply adds the mean of the volume of transactions of a corresponding merchant id to every transaction.\n",
    "    # Could help us get more useful data about the vendor.\n",
    "    \n",
    "    transactions['group_mean'] = transactions['small_group'].map(transactions.groupby('small_group').mean()['amount_rur'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nGAWKI9j5xMy"
   },
   "outputs": [],
   "source": [
    "def process_datetime(transactions, leak_client_id=False):\n",
    "\n",
    "    # Adds various information about the date of the transaction (in the absence of time)\n",
    "    # Also includes various statistics based on the date. \n",
    "    # leak_client_id set to true will add even more statistics\n",
    "    # but will leak the value of client_id into new features\n",
    "\n",
    "    # TODO: add functions for mappings, reducing groups of three lines into one\n",
    "\n",
    "    # converting the date to the datetime type\n",
    "    transactions['trans_date'] = pd.to_datetime(transactions['trans_date'], unit='d')\n",
    "\n",
    "    # processing day of week, finding averages of transactions\n",
    "\n",
    "    transactions['day_of_week'] = transactions['trans_date'].dt.dayofweek\n",
    "\n",
    "    baseline = transactions['day_of_week']\n",
    "    mapping = transactions.groupby('day_of_week').mean()['amount_rur']\n",
    "    transactions['general_day_of_week_transaction_average'] = baseline.map(mapping)\n",
    "\n",
    "    baseline = pd.Series(list(zip(transactions['day_of_week'], transactions['small_group'])))\n",
    "    mapping = transactions.groupby(['day_of_week', 'small_group']).mean()['amount_rur']\n",
    "    transactions['general_day_of_week_transaction_group_average'] = baseline.map(mapping)\n",
    "\n",
    "    if leak_client_id:\n",
    "        baseline = pd.Series(list(zip(transactions['client_id'], transactions['day_of_week'])))\n",
    "        mapping = transactions.groupby(['client_id','day_of_week']).mean()['amount_rur']\n",
    "        transactions['client_day_of_week_transaction_average'] = baseline.map(mapping)\n",
    "\n",
    "        baseline = pd.Series(list(zip(transactions['client_id'], transactions['day_of_week'], transactions['small_group'])))\n",
    "        mapping = transactions.groupby(['client_id','day_of_week', 'small_group']).mean()['amount_rur']\n",
    "        transactions['client_day_of_week_transaction_group_average'] = baseline.map(mapping)\n",
    "\n",
    "    # and daily averages\n",
    "\n",
    "    day_of_week_counts = pd.Series(transactions['trans_date'].unique()).dt.dayofweek.value_counts()\n",
    "    unique_clients = transactions['client_id'].nunique()\n",
    "\n",
    "    baseline = transactions['day_of_week']\n",
    "    mapping = transactions.groupby('day_of_week').sum()['amount_rur']\n",
    "    mapping /= (day_of_week_counts * unique_clients)\n",
    "    transactions['general_day_of_week_daily_average'] = baseline.map(mapping)\n",
    "\n",
    "    baseline = pd.Series(list(zip(transactions['day_of_week'], transactions['small_group'])))\n",
    "    mapping = transactions.groupby(['day_of_week', 'small_group']).sum()['amount_rur']\n",
    "    mapping = mapping.reset_index()\n",
    "    mapping['amount_rur'] /= (mapping['day_of_week'].map(day_of_week_counts) * unique_clients)\n",
    "    mapping = mapping.set_index(['day_of_week', 'small_group'])\n",
    "    mapping = mapping['amount_rur']\n",
    "    transactions['general_day_of_week_daily_group_average'] = baseline.map(mapping)\n",
    "\n",
    "    if leak_client_id:\n",
    "        baseline = pd.Series(list(zip(transactions['client_id'], transactions['day_of_week'])))\n",
    "        mapping = transactions.groupby(['client_id', 'day_of_week']).sum()['amount_rur']\n",
    "        mapping = mapping.reset_index()\n",
    "        mapping['amount_rur'] /= (mapping['day_of_week'].map(day_of_week_counts))\n",
    "        mapping = mapping.set_index(['client_id', 'day_of_week'])\n",
    "        mapping = mapping['amount_rur']\n",
    "        transactions['client_day_of_week_daily_average'] = baseline.map(mapping)\n",
    "\n",
    "        baseline = pd.Series(list(zip(transactions['client_id'], transactions['day_of_week'], transactions['small_group'])))\n",
    "        mapping = transactions.groupby(['client_id', 'day_of_week', 'small_group']).sum()['amount_rur']\n",
    "        mapping = mapping.reset_index()\n",
    "        mapping['amount_rur'] /= (mapping['day_of_week'].map(day_of_week_counts))\n",
    "        mapping = mapping.set_index(['client_id', 'day_of_week', 'small_group'])\n",
    "        mapping = mapping['amount_rur']\n",
    "        transactions['client_day_of_week_daily_group_average'] = baseline.map(mapping)\n",
    "\n",
    "    # processing day of month\n",
    "\n",
    "    # transaction averages\n",
    "\n",
    "    transactions['day_of_month'] = transactions['trans_date'].dt.day\n",
    "\n",
    "    baseline = transactions['day_of_month']\n",
    "    mapping = transactions.groupby('day_of_month').mean()['amount_rur']\n",
    "    transactions['general_day_of_month_transaction_average'] = baseline.map(mapping)\n",
    "\n",
    "    baseline = pd.Series(list(zip(transactions['day_of_month'], transactions['small_group'])))\n",
    "    mapping = transactions.groupby(['day_of_month', 'small_group']).mean()['amount_rur']\n",
    "    transactions['general_day_of_month_transaction_group_average'] = baseline.map(mapping)\n",
    "\n",
    "    if leak_client_id:\n",
    "        baseline = pd.Series(list(zip(transactions['client_id'], transactions['day_of_month'])))\n",
    "        mapping = transactions.groupby(['client_id','day_of_month']).mean()['amount_rur']\n",
    "        transactions['client_day_of_month_transaction_average'] = baseline.map(mapping)\n",
    "\n",
    "        baseline = pd.Series(list(zip(transactions['client_id'], transactions['day_of_month'], transactions['small_group'])))\n",
    "        mapping = transactions.groupby(['client_id','day_of_month', 'small_group']).mean()['amount_rur']\n",
    "        transactions['client_day_of_month_transaction_group_average'] = baseline.map(mapping)\n",
    "\n",
    "    # daily averages\n",
    "\n",
    "    day_of_month_counts = pd.Series(transactions['trans_date'].unique()).dt.day.value_counts()\n",
    "    unique_clients = transactions['client_id'].nunique()\n",
    "\n",
    "    baseline = transactions['day_of_month']\n",
    "    mapping = transactions.groupby('day_of_month').sum()['amount_rur']\n",
    "    mapping /= (day_of_month_counts * unique_clients)\n",
    "    transactions['general_day_of_month_daily_average'] = baseline.map(mapping)\n",
    "\n",
    "    baseline = pd.Series(list(zip(transactions['day_of_month'], transactions['small_group'])))\n",
    "    mapping = transactions.groupby(['day_of_month', 'small_group']).sum()['amount_rur']\n",
    "    mapping = mapping.reset_index()\n",
    "    mapping['amount_rur'] /= (mapping['day_of_month'].map(day_of_month_counts) * unique_clients)\n",
    "    mapping = mapping.set_index(['day_of_month', 'small_group'])\n",
    "    mapping = mapping['amount_rur']\n",
    "    transactions['general_day_of_month_daily_group_average'] = baseline.map(mapping)\n",
    "    \n",
    "    if leak_client_id:\n",
    "        baseline = pd.Series(list(zip(transactions['client_id'], transactions['day_of_month'])))\n",
    "        mapping = transactions.groupby(['client_id', 'day_of_month']).sum()['amount_rur']\n",
    "        mapping = mapping.reset_index()\n",
    "        mapping['amount_rur'] /= (mapping['day_of_month'].map(day_of_month_counts))\n",
    "        mapping = mapping.set_index(['client_id', 'day_of_month'])\n",
    "        mapping = mapping['amount_rur']\n",
    "        transactions['client_day_of_month_daily_average'] = baseline.map(mapping)\n",
    "\n",
    "        baseline = pd.Series(list(zip(transactions['client_id'], transactions['day_of_month'], transactions['small_group'])))\n",
    "        mapping = transactions.groupby(['client_id', 'day_of_month', 'small_group']).sum()['amount_rur']\n",
    "        mapping = mapping.reset_index()\n",
    "        mapping['amount_rur'] /= (mapping['day_of_month'].map(day_of_month_counts))\n",
    "        mapping = mapping.set_index(['client_id', 'day_of_month', 'small_group'])\n",
    "        mapping = mapping['amount_rur']\n",
    "        transactions['client_day_of_month_daily_group_average'] = baseline.map(mapping)\n",
    "\n",
    "    # processing month\n",
    "\n",
    "    # transaction averages\n",
    "\n",
    "    transactions['month'] = transactions['trans_date'].dt.month\n",
    "\n",
    "    baseline = transactions['month']\n",
    "    mapping = transactions.groupby('month').mean()['amount_rur']\n",
    "    transactions['general_month_transaction_average'] = baseline.map(mapping)\n",
    "\n",
    "    baseline = pd.Series(list(zip(transactions['month'], transactions['small_group'])))\n",
    "    mapping = transactions.groupby(['month', 'small_group']).mean()['amount_rur']\n",
    "    transactions['general_month_transaction_group_average'] = baseline.map(mapping)\n",
    "\n",
    "    if leak_client_id:\n",
    "        baseline = pd.Series(list(zip(transactions['client_id'], transactions['month'])))\n",
    "        mapping = transactions.groupby(['client_id','month']).mean()['amount_rur']\n",
    "        transactions['client_month_transaction_average'] = baseline.map(mapping)\n",
    "\n",
    "        baseline = pd.Series(list(zip(transactions['client_id'], transactions['month'], transactions['small_group'])))\n",
    "        mapping = transactions.groupby(['client_id','month', 'small_group']).mean()['amount_rur']\n",
    "        transactions['client_month_transaction_group_average'] = baseline.map(mapping)\n",
    "    \n",
    "\n",
    "    # daily averages\n",
    "\n",
    "    month_counts = pd.Series(transactions['trans_date'].unique()).dt.month.value_counts()\n",
    "    unique_clients = transactions['client_id'].nunique()\n",
    "\n",
    "    baseline = transactions['month']\n",
    "    mapping = transactions.groupby('month').sum()['amount_rur']\n",
    "    mapping /= (month_counts * unique_clients)\n",
    "    transactions['general_month_daily_average'] = baseline.map(mapping)\n",
    "\n",
    "    baseline = pd.Series(list(zip(transactions['month'], transactions['small_group'])))\n",
    "    mapping = transactions.groupby(['month', 'small_group']).sum()['amount_rur']\n",
    "    mapping = mapping.reset_index()\n",
    "    mapping['amount_rur'] /= (mapping['month'].map(month_counts) * unique_clients)\n",
    "    mapping = mapping.set_index(['month', 'small_group'])\n",
    "    mapping = mapping['amount_rur']\n",
    "    transactions['general_month_daily_group_average'] = baseline.map(mapping)\n",
    "\n",
    "    if leak_client_id:\n",
    "        baseline = pd.Series(list(zip(transactions['client_id'], transactions['month'])))\n",
    "        mapping = transactions.groupby(['client_id', 'month']).sum()['amount_rur']\n",
    "        mapping = mapping.reset_index()\n",
    "        mapping['amount_rur'] /= (mapping['month'].map(month_counts))\n",
    "        mapping = mapping.set_index(['client_id', 'month'])\n",
    "        mapping = mapping['amount_rur']\n",
    "        transactions['client_month_daily_average'] = baseline.map(mapping)\n",
    "\n",
    "        baseline = pd.Series(list(zip(transactions['client_id'], transactions['month'], transactions['small_group'])))\n",
    "        mapping = transactions.groupby(['client_id', 'month', 'small_group']).sum()['amount_rur']\n",
    "        mapping = mapping.reset_index()\n",
    "        mapping['amount_rur'] /= (mapping['month'].map(month_counts))\n",
    "        mapping = mapping.set_index(['client_id', 'month', 'small_group'])\n",
    "        mapping = mapping['amount_rur']\n",
    "        transactions['client_month_daily_group_average'] = baseline.map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QQyx6t258-nr"
   },
   "outputs": [],
   "source": [
    "def interval_analysis(transactions, time_intervals):\n",
    "\n",
    "    # Analyses past transactions in given intervals\n",
    "    # Computes slowly and leaks client_id, since it analyses transactions of every client\n",
    "    # time_intervals is a list of strings, each string is a number of days with the letter \"d\" at the end\n",
    "    # example: ['1d', '2d', '7d', '14d', '30d', '365d']\n",
    "\n",
    "    for time_interval in tqdm(time_intervals, desc='Time intervals processed:'):\n",
    "        transactions['past_' + time_interval + '_interval_max'] = 0\n",
    "        transactions['past_' + time_interval + '_interval_avg'] = 0\n",
    "        transactions['past_' + time_interval + '_interval_sum'] = 0\n",
    "        transactions['past_' + time_interval + '_interval_cnt'] = 0\n",
    "        transactions['past_' + time_interval + '_interval_group_max'] = 0\n",
    "        transactions['past_' + time_interval + '_interval_group_avg'] = 0\n",
    "        transactions['past_' + time_interval + '_interval_group_sum'] = 0\n",
    "        transactions['past_' + time_interval + '_interval_group_cnt'] = 0\n",
    "        for client in tqdm(transactions['client_id'].unique(), desc='Clients processed:'):\n",
    "            client_slice_idx = transactions[transactions['client_id'] == client].index\n",
    "            client_slice = transactions.loc[client_slice_idx,:]\n",
    "            transactions.loc[client_slice_idx, 'past_' + time_interval + '_interval_max'] = client_slice.set_index('trans_date')['amount_rur'].rolling(time_interval).max().reset_index()['amount_rur']\n",
    "            transactions.loc[client_slice_idx, 'past_' + time_interval + '_interval_avg'] = client_slice.set_index('trans_date')['amount_rur'].rolling(time_interval).mean().reset_index()['amount_rur']\n",
    "            transactions.loc[client_slice_idx, 'past_' + time_interval + '_interval_sum'] = client_slice.set_index('trans_date')['amount_rur'].rolling(time_interval).sum().reset_index()['amount_rur']\n",
    "            transactions.loc[client_slice_idx, 'past_' + time_interval + '_interval_cnt'] = client_slice.set_index('trans_date')['amount_rur'].rolling(time_interval).count().reset_index()['amount_rur']\n",
    "\n",
    "            for group in client_slice['small_group'].unique():\n",
    "                group_slice_idx = transactions[((transactions['client_id'] == client) & (transactions['small_group'] == group))].index\n",
    "                group_slice = transactions.loc[group_slice_idx,:]\n",
    "                transactions.loc[group_slice_idx, 'past_' + time_interval + '_interval_group_max'] = np.array(group_slice.set_index('trans_date')['amount_rur'].rolling(time_interval).max().reset_index()['amount_rur'])\n",
    "                transactions.loc[group_slice_idx, 'past_' + time_interval + '_interval_group_avg'] = np.array(group_slice.set_index('trans_date')['amount_rur'].rolling(time_interval).mean().reset_index()['amount_rur'])\n",
    "                transactions.loc[group_slice_idx, 'past_' + time_interval + '_interval_group_sum'] = np.array(group_slice.set_index('trans_date')['amount_rur'].rolling(time_interval).sum().reset_index()['amount_rur'])\n",
    "                transactions.loc[group_slice_idx, 'past_' + time_interval + '_interval_group_cnt'] = np.array(group_slice.set_index('trans_date')['amount_rur'].rolling(time_interval).count().reset_index()['amount_rur'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "aBRK_Fb3-Mjz"
   },
   "outputs": [],
   "source": [
    "def process(data, inplace = False, leak_client_id=False):\n",
    "\n",
    "    # Processes the dataset, downsampling, filtering some of the feature values\n",
    "    # and adding new features, based on the EDA conducted earlier.\n",
    "    # If leak_client_id is set to True, features based on client_id will be added to the data.\n",
    "    # This might be unwanted if client_id is the target (like it is in a metric learning task, in a way).\n",
    "\n",
    "    if not inplace:\n",
    "        transactions = data.copy(deep=True)\n",
    "    else:\n",
    "        transactions = data.copy(deep=False)\n",
    "\n",
    "    downsample(transactions, verbose=False)\n",
    "    filter_groups(transactions, threshold_value=0.01, based_on_volume=True)\n",
    "    flag_large_transactions(transactions, quantile=0.996)\n",
    "    group_mean(transactions)\n",
    "    process_datetime(transactions, leak_client_id=False)\n",
    "    \n",
    "    if leak_client_id:\n",
    "        interval_analysis(transactions, time_intervals)\n",
    "        \n",
    "    post_downsample(transactions)\n",
    "\n",
    "    return transactions  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_downsample(transactions, verbose=False):\n",
    "    if verbose:\n",
    "        print('before downsampling:\\n', transactions.dtypes)\n",
    "\n",
    "    for column in transactions.columns:\n",
    "        old_type = transactions[column].dtype\n",
    "        if ((transactions[column].dtype == 'int16') or \n",
    "            (transactions[column].dtype == 'int32') or\n",
    "            (transactions[column].dtype == 'int64')):\n",
    "                transactions[column] = pd.to_numeric(transactions[column], downcast='signed')\n",
    "        if ((transactions[column].dtype == 'float16') or \n",
    "            (transactions[column].dtype == 'float32') or\n",
    "            (transactions[column].dtype == 'float64')):\n",
    "                transactions[column] = pd.to_numeric(transactions[column], downcast='float')\n",
    "        if (verbose) and (old_type != transactions[column].dtype):\n",
    "            print(\"downsampled\", column, \"from\", old_type, \"to\", transactions[column].dtype)\n",
    "    if verbose:\n",
    "        print('after downsampling:\\n', transactions.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tH-p_yOA_gtK"
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# This will take a while. \n",
    "# We have a lot of data, after all.\n",
    "# Get yourself a nice cup of tea while waiting.\n",
    "\n",
    "transactions_train_processed = process(transactions_train)\n",
    "transactions_test_processed = process(transactions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "9z5w1vfOEUpF",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transactions_train_processed.to_csv(path+'transactions_train_processed.csv', index=False)\n",
    "transactions_test_processed.to_csv(path+'transactions_test_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Feature Engineering.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
