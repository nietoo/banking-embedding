{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Supervised.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOfsDXJ5GPp2awi9P9m2/k"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClAXBNzVRwYM"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDPYvv49SMSb"
      },
      "source": [
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "np.set_printoptions(suppress=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZNU4q9KR_7W"
      },
      "source": [
        "Using Google Drive as a storage for the project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__Mm-b0VSzgY",
        "outputId": "e0d6345a-2b15-4266-b61f-c0cfb30c5851"
      },
      "source": [
        "drive.mount('/content/gdrive/')\n",
        "path = 'gdrive/MyDrive/Programming Projects/Embeddings/data/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6jKfowkR5s4"
      },
      "source": [
        "CUDA acceleration (if available)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ijozoo2R4oE"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygvXgNTgR1Ru"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Kg2Q5dew9iO"
      },
      "source": [
        "#HYPERPARAMETERS\n",
        "\n",
        "CHECKPOINT_PATH = path + \"model.pt\"\n",
        "\n",
        "LOAD_PICKLED_DATASET = True\n",
        "\n",
        "SEQUENCE_LEN = 120\n",
        "SEQUENCES_PER_CLIENT = 10\n",
        "\n",
        "EVENT_ENCODING_DIMENSIONS = 32\n",
        "RNN_HIDDEN_STATE_DIMENSIONS = 64\n",
        "CATEGORICAL_EMBEDDING_DIMENSIONS = 8\n",
        "\n",
        "EPOCHS = 64\n",
        "BATCH_SIZE = 512\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "np.random.seed(1871)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtJAzOJnWU8o"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "791gGSQnSQJW"
      },
      "source": [
        "Defining the class for our dataset. We sample each client for TRANSACTION_PER_CLIENT transactions of length SEQUENCE_LEN and set up their respective age groups as targets. \n",
        "\n",
        "Subsequences are generated by choosing a random transaction in the list of client's transactions and taking it and SEQUENCE_LEN - 1 transactions after it. If we run out of transactions, the sequence is padded by zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XusI7iUudVhT"
      },
      "source": [
        "def subsequence_generator(sequence, pad_vector, to_generate=10, length=120):\n",
        "    start_idx = np.random.randint(0, len(sequence) - 1, size=to_generate)\n",
        "    ret = np.tile(pad_vector, (to_generate, length, 1))\n",
        "\n",
        "    for i, idx in enumerate(start_idx):\n",
        "        subsequence = sequence[idx:]\n",
        "        ret[i, 0:subsequence.shape[0]] = subsequence[:min(len(subsequence), length)]\n",
        "\n",
        "    return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwL7VFExLo8O"
      },
      "source": [
        "class TransactionDataset(Dataset):\n",
        "    def __init__(self, data, y, num_subsequences=10, subsequence_len=120):\n",
        "        self.clients = data['client_id'].unique()\n",
        "        client_target_dict = pd.Series(y.bins.values, index=y.client_id).to_dict()\n",
        "\n",
        "        self.target = np.vectorize(client_target_dict.get)(self.clients)\n",
        "\n",
        "\n",
        "        data['small_group'] = data['small_group'] + 1 # allowing us to use \"0\" as padding for embeddings.\n",
        "\n",
        "        data['trans_date'] = pd.to_datetime(data['trans_date'], unit='d')\n",
        "        data['day_of_week'] = data['trans_date'].dt.dayofweek + 1\n",
        "        data['month'] = data['trans_date'].dt.month\n",
        "        data = data.drop(columns=['trans_date'])\n",
        "\n",
        "        data=data.reindex(columns=['client_id','small_group', 'day_of_week', 'month', 'amount_rur'])\n",
        "        \n",
        "        pad_sequence = np.array([0,0,0,0.])\n",
        "\n",
        "\n",
        "        self.data = list()\n",
        "        for i in range(len(self.clients)):\n",
        "            sequence = data[data['client_id'] == self.clients[i]].drop(columns=['client_id']).to_numpy()\n",
        "            self.data.append(subsequence_generator(sequence, pad_sequence, num_subsequences, subsequence_len))\n",
        "        self.data = np.array(self.data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.target.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.target[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AzNmz6XTyix"
      },
      "source": [
        "The class for encoding each individual transaction (event).\n",
        "\n",
        "All categorical features are encoded by an embedding of size EVENT_EMBEDDING_DIMENSIONS.\n",
        "All numerical features are "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoSYG1KtVf-w"
      },
      "source": [
        "class EventEncoder(nn.Module):\n",
        "    def __init__(self, num_features, cat_features, cat_vocab_sizes, out_dim, emb_dim=16):\n",
        "        super(EventEncoder, self).__init__()\n",
        "\n",
        "        self.initial_norm = nn.BatchNorm1d(num_features, affine=False)\n",
        "\n",
        "        self.embeddings = nn.ModuleList() \n",
        "        self.cat_features = cat_features \n",
        "        for i in range(cat_features):\n",
        "            self.embeddings.append(nn.Embedding(cat_vocab_sizes[i], emb_dim, padding_idx=0))\n",
        "\n",
        "        cur_size = emb_dim * cat_features + num_features\n",
        "        self.process = nn.Sequential(\n",
        "            nn.Linear(cur_size, out_dim),\n",
        "            nn.Sigmoid(),\n",
        "            nn.BatchNorm1d(out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, numerical, categorical):\n",
        "        concat_input = self.initial_norm(numerical)\n",
        "\n",
        "        for i in range(self.cat_features):\n",
        "            embedded_cat = self.embeddings[i](categorical[:,i])\n",
        "            concat_input = torch.cat((concat_input, embedded_cat), 1)\n",
        "        out = self.process(concat_input)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4bLCOBdURHS"
      },
      "source": [
        "class SequenceEncoder(nn.Module):\n",
        "    def __init__(self, num_features, cat_features, cat_vocab_sizes, sequence_len, classes, event_dim=32, hidden_size=32, emb_dim=16):\n",
        "\n",
        "        super(SequenceEncoder, self).__init__()\n",
        "\n",
        "        self.num_features = num_features\n",
        "        self.cat_features = cat_features\n",
        "        self.sequence_len = sequence_len\n",
        "        self.event_dim = event_dim\n",
        "\n",
        "        self.event_encoder = EventEncoder(num_features, cat_features, cat_vocab_sizes, event_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.GRU(event_dim, hidden_size, batch_first=True)\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size, classes)\n",
        "                     \n",
        "    def forward(self, numerical, categorical):\n",
        "        # numerical of size (batch_size, num_of_sequences, sequence_len, num_features)\n",
        "        # categorical of size (batch_size, num_of_sequences, sequence_len, cat_features)\n",
        "\n",
        "        numerical = numerical.view(-1, self.num_features)\n",
        "        categorical = categorical.view(-1, self.cat_features)\n",
        "        \n",
        "        # receiving batch_size x num_of_sequences x sequence_len x event_dim events\n",
        "        events = self.event_encoder(numerical, categorical)\n",
        "        events = events.view(-1, self.sequence_len, self.event_dim)\n",
        "\n",
        "        rnn_res = self.rnn(events)[0][:,-1,:]\n",
        "        \n",
        "        out = self.classifier(rnn_res)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlwLecvFMPar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94a3f7b9-2d25-4624-94dd-f77c29e8d9cf"
      },
      "source": [
        "df = pd.read_csv(path + \"transactions_train.csv\")\n",
        "y = pd.read_csv(path + \"train_target.csv\")  \n",
        "\n",
        "if LOAD_PICKLED_DATASET:\n",
        "    train_dataset = pickle.load(open(path + \"supervised_train_dataset.p\", \"rb\" ))\n",
        "    test_dataset = pickle.load(open(path + \"supervised_test_dataset.p\", \"rb\" ))\n",
        "else:\n",
        "    test_clients = np.random.choice(df.client_id.unique(), size=int(0.3 * df.client_id.nunique()), replace=False)\n",
        "    train_clients = np.array(list(set(df.client_id.unique()) - set(test_clients)))\n",
        "    train_idx = ~df.client_id.isin(test_clients)\n",
        "\n",
        "    df_train = df[train_idx].copy()\n",
        "    df_test = df[~train_idx].copy()\n",
        "    train_dataset = TransactionDataset(df_train, \n",
        "                                    y, \n",
        "                                    num_subsequences=SEQUENCES_PER_CLIENT,\n",
        "                                    subsequence_len=SEQUENCE_LEN)\n",
        "    pickle.dump(train_dataset, open(path + \"supervised_train_dataset.p\", \"wb\" ) )\n",
        "\n",
        "    test_dataset = TransactionDataset(df_test, \n",
        "                                    y, \n",
        "                                    num_subsequences=SEQUENCES_PER_CLIENT,\n",
        "                                    subsequence_len=SEQUENCE_LEN)\n",
        "    pickle.dump(test_dataset, open(path + \"supervised_test_dataset.p\", \"wb\" ) )\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(train_features.size())\n",
        "print(train_labels.size())\n",
        "print(len(train_dataset), len(test_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([256, 10, 120, 4])\n",
            "torch.Size([256])\n",
            "21000 9000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv8d5QRpg_cx",
        "outputId": "7679c125-b21b-45c4-cf67-f083f27d9759"
      },
      "source": [
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(train_features.size())\n",
        "print(train_labels.size())\n",
        "print(len(train_dataset), len(test_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([256, 10, 120, 4])\n",
            "torch.Size([256])\n",
            "21000 9000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_4tqVuieypQ"
      },
      "source": [
        "encoder = SequenceEncoder(num_features=1, \n",
        "                          cat_features=3, \n",
        "                          cat_vocab_sizes=[df['small_group'].max() + 2, 8, 13], # +1 for padding; + 1 to consider 0-indexing\n",
        "                          sequence_len=SEQUENCE_LEN, \n",
        "                          classes=y.bins.nunique(), \n",
        "                          event_dim=EVENT_ENCODING_DIMENSIONS,\n",
        "                          hidden_size=RNN_HIDDEN_STATE_DIMENSIONS,\n",
        "                          emb_dim=CATEGORICAL_EMBEDDING_DIMENSIONS)\n",
        "\n",
        "encoder.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(encoder.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "categorical_mask = [True, True, True, False]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgdfqA25xscG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5351744-5cb2-4926-a1cb-85868725c784"
      },
      "source": [
        "train_loss_history = list()\n",
        "val_loss_history = list()\n",
        "\n",
        "train_accuracy_history = list()\n",
        "val_accuracy_history = list()\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "\n",
        "    encoder.train()\n",
        "    cur_losses = list()\n",
        "    cur_accuracy = list()\n",
        "\n",
        "    for batch_idx, (sequences, labels) in enumerate(train_dataloader):\n",
        "        numerical = sequences[:,:,:,np.logical_not(categorical_mask)].to(device)\n",
        "        categorical = sequences[:,:,:,categorical_mask].to(device)\n",
        "        \n",
        "        labels = np.repeat(labels, SEQUENCES_PER_CLIENT).to(device)\n",
        "\n",
        "        embeddings = encoder(numerical.float(), categorical.int()).to(device)\n",
        "        train_loss = loss_func(embeddings, labels)\n",
        "\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        cur_losses.append(train_loss.item())\n",
        "        cur_accuracy.append(accuracy_score(torch.argmax(embeddings.cpu(), dim=1), labels.cpu()))\n",
        "\n",
        "    train_loss_history.append(np.mean(cur_losses))\n",
        "    train_accuracy_history.append(np.mean(cur_accuracy))\n",
        "\n",
        "    encoder.eval()\n",
        "    cur_losses = list()\n",
        "    cur_accuracy = list()\n",
        "\n",
        "    for batch_idx, (sequences, labels) in enumerate(test_dataloader):\n",
        "        numerical = sequences[:,:,:,np.logical_not(categorical_mask)].to(device)\n",
        "        categorical = sequences[:,:,:,categorical_mask].to(device)\n",
        "        \n",
        "        labels = np.repeat(labels, SEQUENCES_PER_CLIENT).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embeddings = encoder(numerical.float(), categorical.int()).to(device)\n",
        "            val_loss = loss_func(embeddings, labels)\n",
        "\n",
        "            cur_losses.append(val_loss.item())\n",
        "            cur_accuracy.append(accuracy_score(torch.argmax(embeddings.cpu(), dim=1), labels.cpu()))\n",
        "\n",
        "    \n",
        "    val_loss_history.append(np.mean(cur_losses))\n",
        "    val_accuracy_history.append(np.mean(cur_accuracy))\n",
        "\n",
        "    print(\"\\nEpoch {}\\n Train Loss = {}\\n Train Accuracy = {}\\n Validation Loss = {}\\n Validation Accuracy = {}\".format(\n",
        "        epoch, train_loss_history[-1], train_accuracy_history[-1], val_loss_history[-1], val_accuracy_history[-1]))\n",
        "\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': encoder.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': train_loss_history[-1],\n",
        "            }, CHECKPOINT_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/64 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  2%|▏         | 1/64 [06:22<6:41:35, 382.47s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 0\n",
            " Train Loss = 1.383726446025343\n",
            " Train Accuracy = 0.2761342243975904\n",
            " Validation Loss = 1.3762504988246493\n",
            " Validation Accuracy = 0.29597873263888885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  3%|▎         | 2/64 [12:43<6:34:50, 382.10s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            " Train Loss = 1.3704510036721287\n",
            " Train Accuracy = 0.308179593373494\n",
            " Validation Loss = 1.365708480278651\n",
            " Validation Accuracy = 0.31115234374999995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  5%|▍         | 3/64 [19:03<6:27:54, 381.55s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2\n",
            " Train Loss = 1.358531272554972\n",
            " Train Accuracy = 0.32190794427710845\n",
            " Validation Loss = 1.3480697439776526\n",
            " Validation Accuracy = 0.3309917534722222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  6%|▋         | 4/64 [25:10<6:17:10, 377.18s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3\n",
            " Train Loss = 1.3351142104849758\n",
            " Train Accuracy = 0.34306758283132527\n",
            " Validation Loss = 1.309004889594184\n",
            " Validation Accuracy = 0.35876953125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  8%|▊         | 5/64 [31:32<6:12:03, 378.36s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4\n",
            " Train Loss = 1.2776587124330452\n",
            " Train Accuracy = 0.38744823042168675\n",
            " Validation Loss = 1.2330971989366744\n",
            " Validation Accuracy = 0.4122395833333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZFeAcO32nlt"
      },
      "source": [
        "plt.figure(figsize=(20,13))\n",
        "\n",
        "plt.plot(train_loss_history, label='Training loss')\n",
        "plt.plot(val_loss_history, label='Validation loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20,13))\n",
        "\n",
        "plt.plot(train_accuracy_history, label='Training accuracy')\n",
        "plt.plot(val_accuracy_history, label='Validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeWCzFesntO5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}